```markdown
# ğŸ“š RAG-based Chatbot (Colab + Hugging Face + FAISS)

This project implements a **Retrieval-Augmented Generation (RAG) chatbot** using **LangChain, Hugging Face models, FAISS, and Google Colab** â€” **completely free** (no OpenAI API key required).

The chatbot answers questions based on the document:
```

data/NLP_with_Transformers.pdf

```

This PDF was used to demonstrate the chatbot's ability to ingest, index, and answer questions from a single reference source.

---

## ğŸš€ Features
- Document ingestion (PDFs in this case) using LangChain loaders  
- Chunking & semantic embeddings using **MiniLM (sentence-transformers)**  
- Vector database using **FAISS** (fully local, free)  
- LLM-based answer generation using **Flan-T5** (Hugging Face)  
- Incremental ingestion: add new PDFs without rebuilding everything  
- Simple **Gradio UI** for chat interface  
- Works entirely in **Google Colab**  

---

## ğŸ› ï¸ Tech Stack
- **LangChain** (document loading, RAG pipeline)  
- **Sentence-Transformers** (`all-MiniLM-L6-v2`) for embeddings  
- **FAISS** for vector similarity search  
- **Hugging Face Transformers** (`flan-t5-base`) for answer generation  
- **Gradio** for chatbot UI  
- **Google Colab** runtime  

---

## ğŸ“‚ Project Structure
```

rag-chatbot/
â”‚â”€â”€ data/
â”‚   â””â”€â”€ NLP_with_Transformers.pdf   # Knowledge source
â”‚
â”‚â”€â”€ notebooks/
â”‚   â””â”€â”€ colab_rag_demo.ipynb        # Main Colab notebook
â”‚
â”‚â”€â”€ src/
â”‚   â”œâ”€â”€ ingest.py                   # Build FAISS index from PDF
â”‚   â”œâ”€â”€ qa_chat.py                  # CLI-based chatbot
â”‚   â”œâ”€â”€ ui.py                       # Gradio UI for chatbot
â”‚
â”‚â”€â”€ requirements.txt                # Dependencies
â”‚â”€â”€ README.md                       # Project description
â”‚â”€â”€ .gitignore                      # Ignore unnecessary files

````

---

## âš¡ Quick Start (Colab)

1. Clone the repo:
```bash
git clone https://github.com/<your-username>/rag-chatbot.git
cd rag-chatbot
````

2. Open `notebooks/colab_rag_demo.ipynb` in **Google Colab**.

3. Ensure your `data/` folder contains the PDF (`NLP_with_Transformers.pdf`).

4. Run all cells â†’ chatbot will be ready in Colab with a Gradio link.

---

## ğŸ§ª Example Usage

**Query:**

```
What is the role of transformers in NLP?
```

**Output:**

```
Transformers enable sequence-to-sequence learning with attention mechanisms, allowing efficient handling of long-range dependencies in NLP tasks.
(Source: NLP_with_Transformers.pdf)
```


## ğŸ”® Future Improvements

* Extend ingestion to multiple PDFs or DOCX files
* Experiment with larger LLMs (Falcon, Llama 2)
* Deploy outside Colab with Streamlit or FastAPI

---
